{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (34799, 32, 32, 3)\n",
      "y_train shape: (34799,)\n",
      "x_test shape: (12630, 32, 32, 3)\n",
      "y_test shape: (12630,)\n"
     ]
    }
   ],
   "source": [
    "# Get all the data from the testing and training data sets\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "# Data set locations and paths\n",
    "trainingPath = \"TrainingData/train.p\"\n",
    "testingPath = \"TrainingData/test.p\"\n",
    "\n",
    "# Get the data from the data sets using pickle to read them in\n",
    "with open(trainingPath, mode=\"rb\") as training_data:\n",
    "    train = pickle.load(training_data)\n",
    "\n",
    "with open(testingPath, mode=\"rb\") as testing_data:\n",
    "    test = pickle.load(testing_data)\n",
    "\n",
    "\n",
    "    \n",
    "# Create the features and labels for the data and link them\n",
    "x_train, y_train = train['features'], train['labels']\n",
    "x_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Training Samples:34799 - Num of Test Sample:12630 - Num Of classes:43\n"
     ]
    }
   ],
   "source": [
    "# Define number of training samples and number of testing samples\n",
    "num_train = len(x_train)\n",
    "num_test = len(x_test)\n",
    "# Define the number of classes (only the unique vales in y sets)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Define the image shape and data shape\n",
    "image_shape = x_train[0].shape\n",
    "print(\"Num of Training Samples:{} - Num of Test Sample:{} - Num Of classes:{}\".format(num_train, num_test, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of label frequency Simple data exploration\n",
    "hist, bins = np.histogram(y_train, bins=num_classes)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB: (34799, 32, 32, 3)\n",
      "Grayscale: (34799, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# Convert to grayscale\n",
    "x_train_rgb = x_train\n",
    "x_train_gry = np.sum(x_train/3, axis=3, keepdims=True)\n",
    "\n",
    "x_test_rgb = x_test\n",
    "x_test_gry = np.sum(x_test/3, axis=3, keepdims=True)\n",
    "\n",
    "print('RGB:', x_train_rgb.shape)\n",
    "print('Grayscale:', x_train_gry.shape)\n",
    "\n",
    "x_train = x_train_gry\n",
    "x_test = x_test_gry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the train and test datasets to (-1,1)\n",
    "\n",
    "x_train_normalized = (x_train - 128)/128 \n",
    "x_test_normalized = (x_test - 128)/128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old X_train size: 34799\n",
      "New X_train size: 27839\n",
      "X_validation size: 6960\n"
     ]
    }
   ],
   "source": [
    "## Shuffle the training dataset and create the validation set\n",
    "# Validatioin set will not be used by Training \n",
    "\n",
    "x_train_normalized, y_train = shuffle(x_train_normalized, y_train)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train_normalized, y_train, \n",
    "                                                                test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Old X_train size:\",len(x_train_normalized))\n",
    "print(\"New X_train size:\",len(x_train))\n",
    "print(\"X_validation size:\",len(x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build LeNET Model\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 100\n",
    "rate = 0.0009 # Learning rate\n",
    "\n",
    "tf.reset_default_graph() \n",
    "# Tensor placehoders \n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1)) # Define shape of a image 32X32\n",
    "y = tf.placeholder(tf.int32, (None))              # Class label\n",
    "keep_prob = tf.placeholder(tf.float32)            # probability to keep units\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma), name=\"W1\")\n",
    "    x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b1 = tf.Variable(tf.zeros(6), name=\"b1\")\n",
    "    x = tf.nn.bias_add(x, b1)\n",
    "    print(\"layer 1 shape:\",x.get_shape())\n",
    "\n",
    "    # Activation function.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Max Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer1 = x\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma), name=\"W2\")\n",
    "    x = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b2 = tf.Variable(tf.zeros(16), name=\"b2\")\n",
    "    x = tf.nn.bias_add(x, b2)\n",
    "                     \n",
    "    # Activation function.\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # Max Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer2 = x\n",
    "    \n",
    "    # Layer 3: Convolutional. Output = 1x1x400.\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 400), mean = mu, stddev = sigma), name=\"W3\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b3 = tf.Variable(tf.zeros(400), name=\"b3\")\n",
    "    x = tf.nn.bias_add(x, b3)\n",
    "                     \n",
    "    # Activation function.\n",
    "    x = tf.nn.relu(x)\n",
    "    layer3 = x\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    layer2flat = flatten(layer2)\n",
    "    print(\"layer2flat shape:\",layer2flat.get_shape())\n",
    "    \n",
    "    # Flatten x. Input = 1x1x400. Output = 400.\n",
    "    xflat = flatten(x)\n",
    "    print(\"xflat shape:\",xflat.get_shape())\n",
    "    \n",
    "    # Concat layer2flat and x. Input = 400 + 400. Output = 800\n",
    "    x = tf.concat([xflat, layer2flat], 1)\n",
    "    print(\"x shape:\",x.get_shape())\n",
    "    \n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 800. Output = 43.\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=(800, 43), mean = mu, stddev = sigma), name=\"W4\")\n",
    "    b4 = tf.Variable(tf.zeros(43), name=\"b4\")    \n",
    "    logits = tf.add(tf.matmul(x, W4), b4)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 shape: (?, 28, 28, 6)\n",
      "layer2flat shape: (?, 400)\n",
      "xflat shape: (?, 400)\n",
      "x shape: (?, 800)\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.426\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.674\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.825\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.878\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.919\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.966\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 11 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 12 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "EPOCH 13 ...\n",
      "Validation Accuracy = 0.986\n",
      "\n",
      "EPOCH 14 ...\n",
      "Validation Accuracy = 0.985\n",
      "\n",
      "EPOCH 15 ...\n",
      "Validation Accuracy = 0.988\n",
      "\n",
      "EPOCH 16 ...\n",
      "Validation Accuracy = 0.992\n",
      "\n",
      "EPOCH 17 ...\n",
      "Validation Accuracy = 0.991\n",
      "\n",
      "EPOCH 18 ...\n",
      "Validation Accuracy = 0.995\n",
      "\n",
      "EPOCH 19 ...\n",
      "Validation Accuracy = 0.995\n",
      "\n",
      "EPOCH 20 ...\n",
      "Validation Accuracy = 0.996\n",
      "\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.995\n",
      "\n",
      "EPOCH 22 ...\n",
      "Validation Accuracy = 0.997\n",
      "\n",
      "EPOCH 23 ...\n",
      "Validation Accuracy = 0.997\n",
      "\n",
      "EPOCH 24 ...\n",
      "Validation Accuracy = 0.998\n",
      "\n",
      "EPOCH 25 ...\n",
      "Validation Accuracy = 0.997\n",
      "\n",
      "EPOCH 26 ...\n",
      "Validation Accuracy = 0.997\n",
      "\n",
      "EPOCH 27 ...\n",
      "Validation Accuracy = 0.998\n",
      "\n",
      "EPOCH 28 ...\n",
      "Validation Accuracy = 0.998\n",
      "\n",
      "EPOCH 29 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 30 ...\n",
      "Validation Accuracy = 0.998\n",
      "\n",
      "EPOCH 31 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 32 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 33 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 34 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 35 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 36 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 37 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 38 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 39 ...\n",
      "Validation Accuracy = 0.998\n",
      "\n",
      "EPOCH 40 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 41 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 42 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 43 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 44 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 45 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 46 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 47 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 48 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 49 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 50 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 51 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 52 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 53 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 54 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 55 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 56 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 57 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 58 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 59 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 60 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(x_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        x_train, y_train = shuffle(x_validation, y_validation)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "            \n",
    "        validation_accuracy = evaluate(x_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess,'.\\lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lenet\n",
      "Test Set Accuracy = 0.911\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver2 = tf.train.import_meta_graph('./lenet.meta')\n",
    "    saver2.restore(sess, \"./lenet\")\n",
    "    test_accuracy = evaluate(x_test_normalized, y_test)\n",
    "    print(\"Test Set Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
