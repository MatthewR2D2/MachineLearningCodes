{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet Archetecture\n",
    "'''\n",
    "Architecture\n",
    "5x5 convolution (32x32x1 in, 28x28x6 out)\n",
    "ReLU\n",
    "2x2 max pool (28x28x6 in, 14x14x6 out)\n",
    "5x5 convolution (14x14x6 in, 10x10x16 out)\n",
    "ReLU\n",
    "2x2 max pool (10x10x16 in, 5x5x16 out)\n",
    "5x5 convolution (5x5x6 in, 1x1x400 out)\n",
    "ReLu\n",
    "Flatten layers from numbers 8 (1x1x400 -> 400) and 6 (5x5x16 -> 400)\n",
    "Concatenate flattened layers to a single size-800 layer\n",
    "Dropout layer\n",
    "Fully connected layer (800 in, 43 out)\n",
    "'''\n",
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b1 = tf.Variable(tf.zeros(6))\n",
    "    x = tf.nn.bias_add(x, b1)\n",
    "    print(\"layer 1 shape:\",x.get_shape())\n",
    "\n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Convolutional. Output = 10x10x16.\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    x = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b2 = tf.Variable(tf.zeros(16))\n",
    "    x = tf.nn.bias_add(x, b2)\n",
    "                     \n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Last thing is to Flatten. Input = 5x5x16. Output = 400.\n",
    "    x = flatten(x)\n",
    "    \n",
    "    # Fully Connected. Input = 400. Output = 120.\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    b3 = tf.Variable(tf.zeros(120))    \n",
    "    x = tf.add(tf.matmul(x, W3), b3)\n",
    "    \n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    b4 = tf.Variable(tf.zeros(84)) \n",
    "    x = tf.add(tf.matmul(x, W4), b4)\n",
    "    \n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    # Fully Connected. Input = 84. Output = 43.\n",
    "    W5 = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    b5 = tf.Variable(tf.zeros(43)) \n",
    "    logits = tf.add(tf.matmul(x, W5), b5)\n",
    "    \n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the place holders \n",
    "tf.reset_default_graph() \n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 shape: (?, 28, 28, 6)\n",
      "WARNING:tensorflow:From <ipython-input-4-1ad2f33536fc>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1ad2f33536fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Define the loss and optimazers here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mloss_operation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m               instructions)\n\u001b[1;32m--> 306\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[0;32m   1945\u001b[0m   \"\"\"\n\u001b[0;32m   1946\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel, labels,\n\u001b[1;32m-> 1947\u001b[1;33m                     logits)\n\u001b[0m\u001b[0;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits_sg\",\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[1;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[0;32m   1757\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[1;32m-> 1759\u001b[1;33m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[0;32m   1760\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "rate = 0.0009 # Training rate\n",
    "\n",
    "logits = LeNet(x)\n",
    "# Define the loss and optimazers here\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
